import errno
import gc
import json
import logging
import os
import pickle
import random
import time
from pathlib import Path
import mlflow
from datetime import datetime

import numpy as np
import pandas as pd
import torch
from easydict import EasyDict as edict
from typing import Dict, List, Union, Optional
from .config import get_config_regression, get_config_tune
from .data_loader import MMDataLoader
from .models import AMIO
from .trains import ATIO
from .utils import assign_gpu, count_parameters, setup_seed

from .models.UHIO import UHIO

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:2" # This is crucial for reproducibility


SUPPORTED_MODELS = [
    'LF_DNN', 'EF_LSTM', 'TFN', 'LMF', 'MFN', 'Graph_MFN', 'MFM',
    'MulT', 'MISA', 'BERT_MAG', 'MLF_DNN', 'MTFN', 'MLMF', 'Self_MM', 'MMIM',
    'MMGPT', 'BiEnc','UniEnc', 'TriEnc', 'msaLM', 'TETFN', 'LF_TRNSF_SUP', 'LF_TRNSF_SSL', 'LF_TRNSF_SFT',
    'TAFFC_O1', 'TAFC_O4', 'TAFC_O5'
]
SUPPORTED_DATASETS = ['MOSI', 'MOSEI', 'SIMS']

logger = logging.getLogger('MMSA')


def _set_logger(log_dir, exp_name, verbose_level):

    # base logger
    log_file_path = Path(log_dir) / f"{exp_name}.log"
    logger = logging.getLogger('MMSA')
    logger.setLevel(logging.DEBUG)

    # file handler
    fh = logging.FileHandler(log_file_path)
    fh_formatter = logging.Formatter('%(asctime)s - %(name)s [%(levelname)s] - %(message)s')
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(fh_formatter)
    logger.addHandler(fh)

    # stream handler
    stream_level = {0: logging.ERROR, 1: logging.INFO, 2: logging.DEBUG}
    ch = logging.StreamHandler()
    ch.setLevel(stream_level[verbose_level])
    ch_formatter = logging.Formatter('%(name)s - %(message)s')
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)

    return logger


def log_config_recursive(config, parent_key='', separator='.'):
    """
    Recursively log configuration parameters to MLflow.
    Handles nested dictionaries, lists, and various data types.
    """
    import mlflow
    
    for key, value in config.items():
        # Create the full parameter name
        full_key = f"{parent_key}{separator}{key}" if parent_key else key
        
        try:
            if isinstance(value, dict):
                # Recursively handle nested dictionaries
                log_config_recursive(value, full_key, separator)
            elif isinstance(value, (list, tuple)):
                # Handle lists and tuples
                if len(value) < 10:  # Only log short lists to avoid clutter
                    mlflow.log_param(full_key, str(value))
                else:
                    # For long lists, log length and first few elements
                    mlflow.log_param(f"{full_key}_length", len(value))
                    mlflow.log_param(f"{full_key}_first_5", str(value[:5]))
            elif isinstance(value, (int, float, str, bool)):
                # Handle basic types
                mlflow.log_param(full_key, value)
            elif hasattr(value, '__str__'):
                # Handle other objects that can be converted to string
                str_value = str(value)
                # MLflow has a 250 char limit for parameter values
                if len(str_value) > 250:
                    mlflow.log_param(full_key, str_value[:247] + "...")
                else:
                    mlflow.log_param(full_key, str_value)
            else:
                # Fallback for complex objects
                mlflow.log_param(full_key, str(type(value)))
                
        except Exception as e:
            # Log the error and continue with other parameters
            print(f"Warning: Could not log parameter {full_key}: {e}")
            try:
                mlflow.log_param(f"{full_key}_error", f"Failed to log: {type(value)}")
            except:
                pass

def MMSA_run(
    model_name: str, dataset_name: str, config_file: str = None,
    config: dict = None, seeds: list = [], is_tune: bool = False,
    tune_times: int = 50, custom_feature: str = None, feature_T: str = None, 
    feature_A: str = None, feature_V: str = None, gpu_ids: list = [0],
    num_workers: int = 1, verbose_level: int = 1,
    model_save_dir: str = "checkpoints",
    res_save_dir: str = "results",
    log_dir: str = "logs",
    exp_name: str = "mult-base",
    fit_head: Optional[bool] = False,
    model_load_path: Optional[str] = "checkpoints",
    lr_head: Optional[List[float]] = [1e-2],
    eval_model: Optional[Dict[str, str]] = {}
):
    """Train and Test MSA models.

    Given a set of hyper-parameters(via config), will train models on training
    and validation set, then test on test set and report the results. If 
    `is_tune` is set, will accept lists as hyper-parameters and conduct a grid
    search to find the optimal values.

    Args:
        model_name: Name of MSA model.
        dataset_name: Name of MSA dataset.
        config_file: Path to config file. If not specified, default config
            files will be used.
        config: Config dict. Used to override arguments in config_file. 
        seeds: List of seeds. Default: [1111, 1112, 1113, 1114, 1115]
        is_tune: Tuning mode switch. Default: False
        tune_times: Sets of hyper parameters to tune. Default: 50
        custom_feature: Path to custom feature file. The custom feature should
            contain features of all three modalities. If only one modality has
            customized features, use `feature_*` below. 
        feature_T: Path to text feature file. Provide an empty string to use
            default BERT features. Default: ""
        feature_A: Path to audio feature file. Provide an empty string to use
            default features provided by dataset creators. Default: ""
        feature_V: Path to video feature file. Provide an empty string to use
            default features provided by dataset creators. Default: ""
        gpu_ids: GPUs to use. Will assign the most memory-free gpu if an empty
            list is provided. Default: [0]. Currently only supports single gpu.
        num_workers: Number of workers used to load data. Default: 4
        verbose_level: Verbose level of stdout. 0 for error, 1 for info, 2 for
            debug. Default: 1
        model_save_dir: Path to save trained model weights. Default: 
            "~/MMSA/saved_models"
        res_save_dir: Path to save csv results. Default: "~/MMSA/results"
        log_dir: Path to save log files. Default: "~/MMSA/logs"
        exp_name: str to identify unique experiment name
        fit_head: boolean, when True fits a linear head on top of each 'unimodal' branch
        model_load_path: for fit head or eval_model only
        lr_head: the learning rate of the linear head
        eval_model: when non empty dict load a model and tests it under the following scenarios
            - "mode": "normal"/"robust"/"dom"
            - "type": "iid"/"corr"/"zero"/"mean", first 2 for robust and second 2 for dom
            - "p": float
    """
    # Initialization
    model_name = model_name.lower()
    dataset_name = dataset_name.lower()

    if config_file is not None:
        config_file = Path(config_file)
    else: # use default config files
        if is_tune:
            config_file = Path(__file__).parent / "config" / "config_tune.json"
        else:
            config_file = Path(__file__).parent / "config" / "config_regression.json"
    if not config_file.is_file():
        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), config_file)
    if exp_name is None:
        exp_name = "mult-base"
    if model_save_dir is None: # use default model save dir
        model_save_dir = "saved_models"
    model_save_dir = f"{model_save_dir}/{exp_name}"
    Path(model_save_dir).mkdir(parents=True, exist_ok=True)
    if res_save_dir is None: # use default result save dir
        res_save_dir = "results"
    res_save_dir = f"{res_save_dir}/{exp_name}"
    Path(res_save_dir).mkdir(parents=True, exist_ok=True)
    if log_dir is None: # use default log save dir
        log_dir = "logs"
    log_dir = f"{log_dir}/{exp_name}"
    Path(log_dir).mkdir(parents=True, exist_ok=True)
    seeds = seeds if seeds != [] else [1111, 1112, 1113, 1114, 1115]
    logger = _set_logger(log_dir, exp_name, verbose_level)

    logger.info("======================================== Program Start ========================================")

    if is_tune: # run tune
        logger.info(f"Tuning with seed {seeds[0]}")
        initial_args = get_config_tune(model_name, dataset_name, config_file)
        initial_args['model_save_path'] = Path(model_save_dir) / f"{initial_args['model_name']}-{initial_args['dataset_name']}.pth"
        initial_args['device'] = assign_gpu(gpu_ids)
        initial_args['train_mode'] = 'regression' # backward compatibility. TODO: remove all train_mode in code
        initial_args['custom_feature'] = custom_feature
        initial_args['feature_T'] = feature_T
        initial_args['feature_A'] = feature_A
        initial_args['feature_V'] = feature_V

        # torch.cuda.set_device() encouraged by pytorch developer, although dicouraged in the doc.
        # https://github.com/pytorch/pytorch/issues/70404#issuecomment-1001113109
        # It solves the bug of RNN always running on gpu 0.
        torch.cuda.set_device(initial_args['device'])

        res_save_dir = Path(res_save_dir) / "tune"
        res_save_dir.mkdir(parents=True, exist_ok=True)
        has_debuged = [] # save used params
        csv_file = res_save_dir / f"{dataset_name}-{model_name}.csv"
        if csv_file.is_file():
            df = pd.read_csv(csv_file)
            for i in range(len(df)):
                has_debuged.append([df.loc[i,k] for k in initial_args['d_paras']])

        for i in range(tune_times):
            args = edict(**initial_args)
            random.seed(time.time())
            new_args = get_config_tune(model_name, dataset_name, config_file)
            args.update(new_args)
            if config:
                if config.get('model_name'):
                    assert(config['model_name'] == args['model_name'])
                args.update(config)
            args['cur_seed'] = i + 1
            logger.info(f"{'-'*30} Tuning [{i + 1}/{tune_times}] {'-'*30}")
            logger.info(f"Args: {args}")
            # check if this param has been run
            cur_param = [args[k] for k in args['d_paras']]
            if cur_param in has_debuged:
                logger.info(f"This set of parameters has been run. Skip.")
                time.sleep(1)
                continue
            # actual running
            setup_seed(seeds[0])
            result = _run(args, num_workers, is_tune)
            has_debuged.append(cur_param)
            # save result to csv file
            if Path(csv_file).is_file():
                df2 = pd.read_csv(csv_file)
            else:
                df2 = pd.DataFrame(columns = [k for k in args.d_paras] + [k for k in result.keys()])
            res = [args[c] for c in args.d_paras]
            for col in result.keys():
                value = result[col]
                res.append(value)
            df2.loc[len(df2)] = res
            df2.to_csv(csv_file, index=None)
            logger.info(f"Results saved to {csv_file}.")
    elif fit_head: # fit unimodal heads on top of each modality
        args = get_config_regression(model_name, dataset_name, config_file)
        args['lr_head'] = lr_head
        args['device'] = assign_gpu(gpu_ids)
        args['train_mode'] = 'regression' # backward compatibility. TODO: remove all train_mode in code
        args['custom_feature'] = custom_feature
        args['feature_T'] = feature_T
        args['feature_A'] = feature_A
        args['feature_V'] = feature_V
        if config: # override some arguments
            if config.get('model_name'):
                assert(config['model_name'] == args['model_name'])
            args.update(config)

        # torch.cuda.set_device() encouraged by pytorch developer, although dicouraged in the doc.
        # https://github.com/pytorch/pytorch/issues/70404#issuecomment-1001113109
        # It solves the bug of RNN always running on gpu 0.
        torch.cuda.set_device(args['device'])

        logger.info("Running with args:")
        logger.info(args)
        logger.info(f"Seeds: {seeds}")
        modalities = ['T', 'A', 'V']
        # modalities = ['Î©']
        # modalities = ['V']
        base_res_save_dir = res_save_dir
        for modal in modalities:
            res_save_dir = Path(base_res_save_dir) / f"uni{modal}"
            res_save_dir.mkdir(parents=True, exist_ok=True)
            model_results = []
            for i, seed in enumerate(seeds):
                # assume to already have 'model_load_path': args['model_load_path']
                args['model_load_path'] = model_load_path
                print(f"Loading checkpoint from {args['model_load_path']}")
                args['model_save_path'] = \
                    Path(model_save_dir)/f"{args['model_name']}-{modal}-head-{args['dataset_name']}-{str(seed)}.pth"
                setup_seed(seed)
                args['cur_seed'] = i + 1
                logger.info(f"{'-'*30} Running with seed {seed} [{i + 1}/{len(seeds)}] {'-'*30}")
                # fit linear head on top of 'modal' modality
                result = _fit(args, num_workers, modal)
                result['seed'] = seed
                logger.info(f"Result for seed {seed}: {result}")
                model_results.append(result)
            criterions = list(model_results[0].keys())
            avg_criterions = []
            for k in criterions:
                if k != 'seed':
                    avg_criterions.append(k)
            # save average results to csv
            avg_csv_file = res_save_dir / f"{dataset_name}_avg.csv"
            if avg_csv_file.is_file():
                df = pd.read_csv(avg_csv_file)
            else:
                df = pd.DataFrame(columns=["Model"] + avg_criterions)
            # save all results to csv
            all_csv_file = res_save_dir / f"{dataset_name}_all.csv"
            if all_csv_file.is_file():
                df_all = pd.read_csv(all_csv_file)
                df_all = pd.concat([df_all, pd.DataFrame(criterions)])
            else:
                df_all = pd.DataFrame(model_results)
            df_all.to_csv(all_csv_file, index=None)
            # save results
            res = [model_name]
            for c in avg_criterions:
                values = [r[c] for r in model_results]
                mean = round(np.mean(values)*100, 2)
                std = round(np.std(values)*100, 2)
                res.append((mean, std))
            df.loc[len(df)] = res
            df.to_csv(avg_csv_file, index=None)
            logger.info(f"Results saved to {avg_csv_file}.")
    elif eval_model: # evaluation setups
        # eval_model["mode"]: ["eval", "robust", "dominance"]
        # eval_model["type"]: ["eval", "iid/corr", "mean/zero"]
        # eval_model["p"]: [1.0, float, "1.0/float"]
        # eval_model["runs"]: [1, int, "1/int"]
        test_mode = eval_model["mode"]
        test_type = eval_model["type"]
        test_prob = eval_model.get("p", 1)
        args = get_config_regression(model_name, dataset_name, config_file)
        args['device'] = assign_gpu(gpu_ids)
        args['train_mode'] = 'regression' # backward compatibility. TODO: remove all train_mode in code
        args['custom_feature'] = custom_feature
        args['feature_T'] = feature_T
        args['feature_A'] = feature_A
        args['feature_V'] = feature_V
        if config: # override some arguments
            if config.get('model_name'):
                assert(config['model_name'] == args['model_name'])
            args.update(config)

        # torch.cuda.set_device() encouraged by pytorch developer, although dicouraged in the doc.
        # https://github.com/pytorch/pytorch/issues/70404#issuecomment-1001113109
        # It solves the bug of RNN always running on gpu 0.
        torch.cuda.set_device(args['device'])

        logger.info("Running with args:")
        logger.info(args)
        res_save_dir = Path(res_save_dir) / f"{test_mode}_{test_type}_{test_prob}"
        res_save_dir.mkdir(parents=True, exist_ok=True)
        model_results = []
        base_seed = 2023
        args['model_load_path'] = model_load_path
        for n_run in range(eval_model["runs"]):
            # assume to already have 'model_load_path': args['model_load_path']
            print(f"Loading checkpoint from {args['model_load_path']}")    
            seed = base_seed + n_run
            setup_seed(seed)
            args['cur_seed'] = n_run + 1
            result = _eval(args, num_workers, eval_model)
            result['seed'] = seed
            logger.info(f"Result for seed {seed}: {result}")
            model_results.append(result)
        criterions = list(model_results[0].keys())
        avg_criterions = []
        for k in criterions:
            if k != 'seed':
                avg_criterions.append(k)
        # save average results to csv
        avg_csv_file = res_save_dir / f"{dataset_name}_avg.csv"
        if avg_csv_file.is_file():
            df = pd.read_csv(avg_csv_file)
        else:
            df = pd.DataFrame(columns=["Model"] + avg_criterions)
        # save all results to csv
        all_csv_file = res_save_dir / f"{dataset_name}_all.csv"
        if all_csv_file.is_file():
            df_all = pd.read_csv(all_csv_file)
            df_all = pd.concat([df_all, pd.DataFrame(criterions)])
        else:
            df_all = pd.DataFrame(model_results)
        df_all.to_csv(all_csv_file, index=None)
        # save results
        res = [model_name]
        for c in avg_criterions:
            values = [r[c] for r in model_results]
            mean = round(np.mean(values)*100, 2)
            std = round(np.std(values)*100, 2)
            res.append((mean, std))
        df.loc[len(df)] = res
        df.to_csv(avg_csv_file, index=None)
        logger.info(f"Results saved to {avg_csv_file}.")
    else: # run normal
        args = get_config_regression(model_name, dataset_name, config_file)
        # TODO: uncomment
        args['device'] = assign_gpu(gpu_ids)
        args['train_mode'] = 'regression' # backward compatibility. TODO: remove all train_mode in code
        args['custom_feature'] = custom_feature
        args['feature_T'] = feature_T
        args['feature_A'] = feature_A
        args['feature_V'] = feature_V
        if config: # override some arguments
            if config.get('model_name'):
                assert(config['model_name'] == args['model_name'])
            args.update(config)

        # torch.cuda.set_device() encouraged by pytorch developer, although dicouraged in the doc.
        # https://github.com/pytorch/pytorch/issues/70404#issuecomment-1001113109
        # It solves the bug of RNN always running on gpu 0.
        # TODO: uncomment
        torch.cuda.set_device(args['device'])

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        run_name = f"run_{timestamp}"
        mlflow.set_tracking_uri("file:///leonardo/home/userexternal/nxiros00/Deepmlf/mlflow/mlruns")
        #MLFLOW NAME
        mlflow.set_experiment("mega-experiments")
        mlflow.start_run(run_name=run_name)
        
        # Log all config parameters
        try:
            log_config_recursive(args)
            print(f"Successfully logged config parameters to MLflow")
        except Exception as e:
            print(f"Error logging config: {e}")

        logger.info("Running with args:")
        logger.info(args)
        logger.info(f"Seeds: {seeds}")
        res_save_dir = Path(res_save_dir) / "normal"
        res_save_dir.mkdir(parents=True, exist_ok=True)
        model_results = []
        for i, seed in enumerate(seeds):
            if args['model_name'] == "unienc" or args['model_name'] == "bienc" or args['model_name'] == "trienc":
                # Extract additional parameters from the nested unienc config
                if args['model_name'] == "unienc":
                    unienc_config = args.get('unienc', {})
                    d_enc = unienc_config.get('d_enc', 'N/A')
                    n_head = unienc_config.get('n_head', 'N/A')
                    nlevels = unienc_config.get('nlevels', 'N/A')
                    maxlen = unienc_config.get('maxlen', 'N/A')
                elif args['model_name'] == "bienc":
                    bienc_config = args.get('av_enc', {})
                    d_enc = bienc_config.get('d_enc', 'N/A')
                    n_head = bienc_config.get('n_head', 'N/A')
                    nlevels = bienc_config.get('nlevels', 'N/A')
                    maxlen = bienc_config.get('maxlen', 'N/A')

                # Create filename with additional parameters
                args['model_save_path'] = \
                    Path(model_save_dir)/f"{args['model_name']}-{args['dataset_name']}-{str(seed)}-d_enc_{d_enc}-n_head_{n_head}-nlevels_{nlevels}-maxlen_{maxlen}.pth"
            else:
                # Use original naming convention
                args['model_save_path'] = \
                    Path(model_save_dir)/f"{args['model_name']}-{args['dataset_name']}-{str(seed)}.pth"
            setup_seed(seed)
            args['cur_seed'] = i + 1
            logger.info(f"{'-'*30} Running with seed {seed} [{i + 1}/{len(seeds)}] {'-'*30}")
            # actual running
            result = _run(args, num_workers, is_tune)
            result['seed'] = seed
            logger.info(f"Result for seed {seed}: {result}")
            model_results.append(result)
        criterions = list(model_results[0].keys())
        avg_criterions = []
        for k in criterions:
            if k != 'seed':
                avg_criterions.append(k)
        
        #MLFLOW LOGGING
        try:
            # Log final averaged results
            for c in avg_criterions:
                values = [r[c] for r in model_results]
                mean = round(np.mean(values)*100, 2)
                std = round(np.std(values)*100, 2)
                
                # Log both mean and std
                mlflow.log_metric(f"{c}_mean", mean)
                mlflow.log_metric(f"{c}_std", std)
            
            # Log additional info
            mlflow.log_param("num_seeds", len(seeds))
            mlflow.log_param("seeds_used", str(seeds))
            
            print(f"Successfully logged final averaged results to MLflow")
            
        except Exception as e:
            print(f"Error logging final results: {e}")
        
        # End MLflow run
        try:
            mlflow.end_run()
            print("MLflow run ended successfully")
        except Exception as e:
            print(f"Error ending MLflow run: {e}")

        # save average results to csv
        avg_csv_file = res_save_dir / f"{dataset_name}_avg.csv"
        
        # Extract config parameters you want to save
        config_params = []
        config_values = []

        # Define the config parameters to extract
        params_to_extract = [
            'learning_rate_mmgpt',
            'attention_pattern',
            'nv_bn_fusion',
            'na_bn_fusion',
            'n_bn_fusion',
            'accumilation',
            'd_enc_a',
            'd_enc_v',
            'd_enc_av',
            'maxlen_a',
            'maxlen_v',
            'maxlen_av',
            'mm_layer',
        ]

        # Extract values from args, handling the actual nested structure from your config
        for param in params_to_extract:
            try:
                if param == 'learning_rate_mmgpt':
                    value = args.get('learning_rate_mmgpt', 'N/A')
                elif param == 'attention_pattern':
                    value = args.get('mmgpt', {}).get('attention_pattern', 'N/A')
                elif param in ['nv_bn_fusion', 'na_bn_fusion', 'n_bn_fusion', 'accumilation']:
                    value = args.get(param, 'N/A')

                # d_enc parameters
                elif param == 'd_enc_a':
                    value = args.get('av_enc', {}).get('audio_encoder', {}).get('d_enc', 'N/A')
                elif param == 'd_enc_v':
                    value = args.get('av_enc', {}).get('vision_encoder', {}).get('d_enc', 'N/A')
                elif param == 'd_enc_av':
                    value = args.get('av_enc', {}).get('bimodal_encoder', {}).get('d_enc', 'N/A')

                # maxlen parameters
                elif param == 'maxlen_a':
                    value = args.get('av_enc', {}).get('audio_encoder', {}).get('maxlen', 'N/A')
                elif param == 'maxlen_v':
                    value = args.get('av_enc', {}).get('vision_encoder', {}).get('maxlen', 'N/A')
                elif param == 'maxlen_av':
                    value = args.get('av_enc', {}).get('bimodal_encoder', {}).get('maxlen', 'N/A')


                elif param == 'mm_layer':
                    value = args.get('mmgpt', {}).get('mm_layer', 'N/A')
                else:
                    value = 'N/A'
            except (KeyError, AttributeError, TypeError):
                value = 'N/A'
            
            config_params.append(param)
            config_values.append(value)

        if avg_csv_file.is_file():
            df = pd.read_csv(avg_csv_file)
        else:
            # Create columns: Model + config params + performance metrics
            df = pd.DataFrame(columns=["Model"] + config_params + avg_criterions)

        # save all results to csv
        all_csv_file = res_save_dir / f"{dataset_name}_all.csv"
        if all_csv_file.is_file():
            df_all = pd.read_csv(all_csv_file)
            df_all = pd.concat([df_all, pd.DataFrame(criterions)])
        else:
            df_all = pd.DataFrame(model_results)
        df_all.to_csv(all_csv_file, index=None)

        # save results with config parameters
        try:
            res = [model_name] + config_values
            for c in avg_criterions:
                values = [r[c] for r in model_results]
                mean = round(np.mean(values)*100, 2)
                std = round(np.std(values)*100, 2)
                res.append((mean, std))
            
            df.loc[len(df)] = res
        except ValueError as e:
            logger.warning(f"Skipping model {model_name} due to column mismatch: {e}")
            # Optionally, you could create a separate DataFrame for these models
        df.to_csv(avg_csv_file, index=None)
        logger.info(f"Results saved to {avg_csv_file}.")


def _run(args, num_workers=1, is_tune=False, from_sena=False):
    
    # load data and models
    dataloader = MMDataLoader(args, num_workers)
    model = AMIO(args).to(args['device'])
    
    ###############################################################################################
    ## for pkoro experiments
    # @efthygeo comment: REMOVE AFTER SFT experiments
    if args.get("load_ssl", False):
        model.load_state_dict(torch.load(args['ssl_model_path']))
        
    logger.info(f'The model has {count_parameters(model)} trainable parameters')
    
    # Log model parameters count
    if not is_tune and not from_sena:
        try:
            mlflow.log_param("model_parameters", count_parameters(model))
        except:
            pass
    
    trainer = ATIO().getTrain(args)
    
    # do train
    epoch_results = trainer.do_train(model, dataloader, return_epoch_results=from_sena)
    
    # load trained model & do test
    assert Path(args['model_save_path']).exists()
    print(f"***************** Loading Model from {args['model_save_path']}")
    model.load_state_dict(torch.load(args['model_save_path']))
    model.to(args['device'])
    
    if from_sena:
        final_results = {}
        final_results['train'] = trainer.do_test(model, dataloader['train'], mode="TRAIN", return_sample_results=True)
        final_results['valid'] = trainer.do_test(model, dataloader['valid'], mode="VALID", return_sample_results=True)
        final_results['test'] = trainer.do_test(model, dataloader['test'], mode="TEST", return_sample_results=True)
    elif is_tune:
        results = trainer.do_test(model, dataloader['test'], mode="TEST")
        # delete saved model
        Path(args['model_save_path']).unlink(missing_ok=True)
    else:
        ###########################################################################################
        ## for pkoro experiments
        if args.get("ssl", False):
            results = {}
        else:
            results = trainer.do_test(model, dataloader['test'], mode="TEST")
    
    if args.get('del_model', False):
        print(f"Deleting stored model from {args['model_save_path']}")
        try:
            os.remove(args['model_save_path'])
        except Exception as e:
            print(f"Warning: Could not delete model file: {e}")
    
    del model
    torch.cuda.empty_cache()
    gc.collect()
    time.sleep(1)
    
    return {"epoch_results": epoch_results, 'final_results': final_results} if from_sena else results


def _fit(args, num_workers=1, modal='T'):
    '''
    Function which fits linear head on top of a pretrained model
    '''
    # load data and models
    dataloader = MMDataLoader(args, num_workers)
    backbone = AMIO(args).to(args['device'])
    backbone.load_state_dict(torch.load(args['model_load_path']))
    model = UHIO(args, modal=modal).to(args['device'])

    logger.info(f'The backbone has {count_parameters(backbone)} parameters')
    logger.info(f'The model has {count_parameters(model)} trainable parameters')
    # TODO: use multiple gpus
    # if using_cuda and len(args.gpu_ids) > 1:
    #     model = torch.nn.DataParallel(model,
    #                                   device_ids=args.gpu_ids,
    #                                   output_device=args.gpu_ids[0])
    trainer = ATIO().getTrain(args)
    # do train
    epoch_results = \
        trainer.do_fit_head(backbone, model, dataloader, modal=modal)
    # load trained model & do test
    assert Path(args['model_save_path']).exists()
    model.load_state_dict(torch.load(args['model_save_path']))
    model.to(args['device'])
    results = \
        trainer.do_test_head(backbone, model, dataloader['test'], mode="TEST", modal=modal)

    del model
    torch.cuda.empty_cache()
    gc.collect()
    time.sleep(1)

    return results


def _eval(args, num_workers=1, eval_model=None):
    '''
    Function which evaluates a pretrained model
    '''
    # load data and models
    dataloader = MMDataLoader(args, num_workers)
    model = AMIO(args).to(args['device'])
    model.load_state_dict(torch.load(args['model_load_path']))
    model.to(args['device'])

    logger.info(f'The model has {count_parameters(model)} parameters')
    trainer = ATIO().getTrain(args)
    # do eval
    if eval_model["mode"] == "eval":
        results = trainer.do_test(model, dataloader['test'], mode="TEST")
    elif eval_model["mode"] == "robust":
        results = trainer.do_robust(model, dataloader['test'], eval_model["type"], eval_model["p"])
    elif eval_model["mode"] == "dom":
        # todo
        results = trainer.do_dominance(
            model, dataloader['test'],
            eval_model["type"], eval_model["p"],
            dataloader["train"]
        )
    else:
        raise KeyError(f"The evaluation protocol is not valid. Please check your input args")

    del model
    torch.cuda.empty_cache()
    gc.collect()
    time.sleep(1)

    return results


def MMSA_test(
    config: dict | str,
    weights_path: str,
    feature_path: str, 
    # seeds: list = [], 
    gpu_id: int = 0, 
):
    """Test MSA models on a single sample.

    Load weights and configs of a saved model, input pre-extracted
    features of a video, then get sentiment prediction results.

    Args:
        model_name: Name of MSA model.
        config: Config dict or path to config file.
        weights_path: Pkl file path of saved model weights.
        feature_path: Pkl file path of pre-extracted features.
        gpu_id: Specify which gpu to use. Use cpu if value < 0.
    """
    if type(config) == str or type(config) == Path:
        config = Path(config)
        with open(config, 'r') as f:
            args = json.load(f)
    elif type(config) == dict or type(config) == edict:
        args = config
    else:
        raise ValueError(f"'config' should be string or dict, not {type(config)}")
    args['train_mode'] = 'regression' # backward compatibility.

    if gpu_id < 0:
        device = torch.device('cpu')
    else:
        device = torch.device(f'cuda:{gpu_id}')
    args['device'] = device
    with open(feature_path, "rb") as f:
        feature = pickle.load(f)
    args['feature_dims'] = [feature['text'].shape[1], feature['audio'].shape[1], feature['vision'].shape[1]]
    args['seq_lens'] = [feature['text'].shape[0], feature['audio'].shape[0], feature['vision'].shape[0]]
    model = AMIO(args)
    model.load_state_dict(torch.load(weights_path), strict=False)
    model.to(device)
    model.eval()
    with torch.no_grad():
        if args.get('use_bert', None):
            if type(text := feature['text_bert']) == np.ndarray:
                text = torch.from_numpy(text).float()
        else:
            if type(text := feature['text']) == np.ndarray:
                text = torch.from_numpy(text).float()
        if type(audio := feature['audio']) == np.ndarray:
            audio = torch.from_numpy(audio).float()
        if type(vision := feature['vision']) == np.ndarray:
            vision = torch.from_numpy(vision).float()
        text = text.unsqueeze(0).to(device)
        audio = audio.unsqueeze(0).to(device)
        vision = vision.unsqueeze(0).to(device)
        if args.get('need_normalized', None):
            audio = torch.mean(audio, dim=1, keepdims=True)
            vision = torch.mean(vision, dim=1, keepdims=True)
        # TODO: write a do_single_test function for each model in trains
        if args['model_name'] == 'self_mm' or args['model_name'] == 'mmim':
            output = model(text, (audio, torch.tensor(audio.shape[1]).unsqueeze(0)), (vision, torch.tensor(vision.shape[1]).unsqueeze(0)))
        elif args['model_name'] == 'tfr_net':
            input_mask = torch.tensor(feature['text_bert'][1]).unsqueeze(0).to(device)
            output, _ = model((text, text, None), (audio, audio, input_mask, None), (vision, vision, input_mask, None))
        else:
            output = model(text, audio, vision)
        if type(output) == dict:
            output = output['M']
    return output.cpu().detach().numpy()[0][0]


SENA_ENABLED = True
try:
    from datetime import datetime
    from multiprocessing import Queue

    import mysql.connector
    from sklearn.decomposition import PCA
except ImportError:
    logger.debug("SENA_run is not loaded due to missing dependencies. Ignore this if you are not using M-SENA Platform.")
    SENA_ENABLED = False

if SENA_ENABLED:
    def SENA_run(
        task_id: int, progress_q: Queue, db_url: str,
        parameters: str, model_name: str, dataset_name: str,
        is_tune: bool, tune_times: int,
        feature_T: str, feature_A: str, feature_V: str,
        model_save_dir: str, res_save_dir: str, log_dir: str,
        gpu_ids: list, num_workers: int, seed: int, desc: str
    ) -> None:
        """
        Run M-SENA tasks. Should only be called from M-SENA Platform.
        Run only one seed at a time.

        Parameters:
            task_id (int): Task id.
            progress_q (multiprocessing.Queue): Used to communicate with M-SENA Platform.
            db_url (str): Database url.
            parameters (str): Training parameters in JSON.
            model_name (str): Model name.
            dataset_name (str): Dataset name.
            is_tune (bool): Whether to tune hyper parameters.
            tune_times (int): Number of times to tune hyper parameters.
            feature_T (str): Path to text feature file.
            feature_A (str): Path to audio feature file.
            feature_V (str): Path to video feature file.
            model_save_dir (str): Path to save trained model.
            res_save_dir (str): Path to save training results.
            log_dir (str): Path to save training logs.
            gpu_ids (list): GPU ids.
            num_workers (int): Number of workers.
            seed (int): Only one seed.
            desc (str): Description.
        """
        # TODO: add progress report
        cursor = None
        try:
            logger = logging.getLogger('app')
            logger.info(f"M-SENA Task {task_id} started.")
            time.sleep(1) # make sure task status is committed by the parent process
            # get db parameters
            db_params = db_url.split('//')[1].split('@')[0].split(':')
            db_user = db_params[0]
            db_pass = db_params[1]
            db_params = db_url.split('//')[1].split('@')[1].split('/')
            db_host = db_params[0]
            db_name = db_params[1]
            # connect to db
            db = mysql.connector.connect(
                user=db_user, password=db_pass, host=db_host, database=db_name
            )
            cursor = db.cursor()
            # load training parameters
            if parameters == "": # use default config file
                if is_tune: # TODO
                    config_file = Path(__file__).parent / "config" / "config_tune.json"
                    args = get_config_tune(model_name, dataset_name, config_file)
                else:
                    config_file = Path(__file__).parent / "config" / "config_regression.json"
                    args = get_config_regression(model_name, dataset_name, config_file)
            else: # load from JSON
                args = json.loads(parameters)
                args['model_name'] = model_name
                args['dataset_name'] = dataset_name
            args['feature_T'] = feature_T
            args['feature_A'] = feature_A
            args['feature_V'] = feature_V
            # determine feature_dims
            if args['feature_T']:
                with open(args['feature_T'], 'rb') as f:
                    data_T = pickle.load(f)
                if 'use_bert' in args and args['use_bert']:
                    args['feature_dims'][0] = 768
                else:
                    args['feature_dims'][0] = data_T['valid']['text'].shape[2]
            if args['feature_A']:
                with open(args['feature_A'], 'rb') as f:
                    data_A = pickle.load(f)
                args['feature_dims'][1] = data_A['valid']['audio'].shape[2]
            if args['feature_V']:
                with open(args['feature_V'], 'rb') as f:
                    data_V = pickle.load(f)
                args['feature_dims'][2] = data_V['valid']['vision'].shape[2]
            args['device'] = assign_gpu(gpu_ids)
            args['cur_seed'] = 1 # the _run function need this to print log
            args['train_mode'] = 'regression' # backward compatibility. TODO: remove all train_mode in code
            args = edict(args)
            # create folders
            Path(model_save_dir).mkdir(parents=True, exist_ok=True)
            Path(res_save_dir).mkdir(parents=True, exist_ok=True)
            Path(log_dir).mkdir(parents=True, exist_ok=True)
            # create db record
            args_dump = args.copy()
            args_dump['device'] = str(args_dump['device'])
            custom_feature = False if (feature_A == "" and feature_V == "" and feature_T == "") else True
            cursor.execute(
                """
                    INSERT INTO Result (dataset_name, model_name, is_tune, custom_feature, created_at,
                     args, save_model_path, loss_value, accuracy, f1, mae, corr, description)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """,
                (dataset_name, model_name, is_tune, custom_feature, datetime.now(), json.dumps(args_dump), '', 0, 0, 0, 0, 0, desc)
            )
            result_id = cursor.lastrowid
            # result_id is allocated now, so model_save_path can be determined
            args['model_save_path'] = Path(model_save_dir) / f"{args['model_name']}-{args['dataset_name']}-{result_id}.pth"
            cursor.execute(
                "UPDATE Result SET save_model_path = %s WHERE result_id = %s", (str(args['model_save_path']), result_id)
            )
            # start training
            try:
                torch.cuda.set_device(args['device'])
                logger.info(f"Running with seed {seed}:")
                logger.info(f"Args:\n{args}")
                setup_seed(seed)
                # actual training
                results_dict = _run(args, num_workers, is_tune, from_sena=True)
                # db operations
                sample_dict = {}
                cursor2 = db.cursor(named_tuple=True)
                cursor2.execute("SELECT * FROM Dsample WHERE dataset_name=%s", (dataset_name,))
                samples = cursor2.fetchall()
                for sample in samples:
                    key = sample.video_id + '$_$' + sample.clip_id
                    sample_dict[key] = (sample.sample_id, sample.annotation)
                # update final results of test set
                cursor.execute(
                    """UPDATE Result SET loss_value = %s, accuracy = %s, f1 = %s,
                    mae = %s, corr = %s WHERE result_id = %s""",
                    (
                        results_dict['final_results']['test']['Loss'],
                        results_dict['final_results']['test']['Non0_acc_2'],
                        results_dict['final_results']['test']['Non0_F1_score'],
                        results_dict['final_results']['test']['MAE'],
                        results_dict['final_results']['test']['Corr'],
                        result_id
                    )
                )
                # save features
                if not is_tune:
                    logger.info("Running feature PCA ...")
                    features = {
                        k: results_dict['final_results'][k]['Features']
                        for k in ['train', 'valid', 'test']
                    }
                    all_features = {}
                    for select_modes in [['train', 'valid', 'test'], ['train', 'valid'], ['train', 'test'], \
                                        ['valid', 'test'], ['train'], ['valid'], ['test']]:
                        # create label index dict
                        # {"Negative": [1,2,5,...], "Positive": [...], ...}
                        cur_labels = []
                        for mode in select_modes:
                            cur_labels.extend(results_dict['final_results'][mode]['Labels'])
                        cur_labels = np.array(cur_labels)
                        label_index_dict = {}
                        label_index_dict['Negative'] = np.where(cur_labels < 0)[0].tolist()
                        label_index_dict['Neutral'] = np.where(cur_labels == 0)[0].tolist()
                        label_index_dict['Positive'] = np.where(cur_labels > 0)[0].tolist()
                        # handle features
                        cur_mode_features_2d = {}
                        cur_mode_features_3d = {}
                        for name in ['Feature_t', 'Feature_a', 'Feature_v', 'Feature_f']: 
                            cur_features = []
                            for mode in select_modes:
                                if name in features[mode]:
                                    cur_features.append(features[mode][name])
                            if cur_features != []:
                                cur_features = np.concatenate(cur_features, axis=0)
                            # PCA analysis
                            if len(cur_features) != 0:
                                pca=PCA(n_components=3, whiten=True)
                                features_3d = pca.fit_transform(cur_features)
                                # split by labels
                                cur_mode_features_3d[name] = {}
                                for k, v in label_index_dict.items():
                                    cur_mode_features_3d[name][k] = features_3d[v].tolist()
                                # PCA analysis
                                pca=PCA(n_components=2, whiten=True)
                                features_2d = pca.fit_transform(cur_features)
                                # split by labels
                                cur_mode_features_2d[name] = {}
                                for k, v in label_index_dict.items():
                                    cur_mode_features_2d[name][k] = features_2d[v].tolist()
                        all_features['-'.join(select_modes)] = {'2D': cur_mode_features_2d, '3D': cur_mode_features_3d}
                    # save features
                    save_path = args.model_save_path.parent / (args.model_save_path.stem + '.pkl')
                    with open(save_path, 'wb') as fp:
                        pickle.dump(all_features, fp, protocol = 4)
                    logger.info(f'Feature saved at {save_path}.')
                # update sample results
                for mode in ['train', 'valid', 'test']:
                    final_results = results_dict['final_results'][mode]
                    for i, cur_id in enumerate(final_results["Ids"]):
                        cursor.execute(
                            """ INSERT INTO SResults (result_id, sample_id, label_value, predict_value, predict_value_r)
                            VALUES (%s, %s, %s, %s, %s)""",
                            (result_id, sample_dict[cur_id][0], sample_dict[cur_id][1],
                            'Negative' if final_results["SResults"][i] < 0 else 'Positive',
                            float(final_results["SResults"][i]))
                        )
                # update epoch results
                cur_results = {}
                for mode in ['train', 'valid', 'test']:
                    cur_epoch_results = results_dict['final_results'][mode]
                    cur_results[mode] = {
                        "loss_value":cur_epoch_results["Loss"],
                        "accuracy":cur_epoch_results["Non0_acc_2"],
                        "f1":cur_epoch_results["Non0_F1_score"],
                        "mae":cur_epoch_results["MAE"],
                        "corr":cur_epoch_results["Corr"]
                    }
                cursor.execute(
                    "INSERT INTO EResult (result_id, epoch_num, results) VALUES (%s, %s, %s)",
                    (result_id, -1, json.dumps(cur_results))
                )

                epoch_num = len(results_dict['epoch_results']['train'])
                for i in range(0, epoch_num):
                    cur_results = {}
                    for mode in ['train', 'valid', 'test']:
                        cur_epoch_results = results_dict['epoch_results'][mode][i]
                        cur_results[mode] = {
                            "loss_value":cur_epoch_results["Loss"],
                            "accuracy":cur_epoch_results["Non0_acc_2"],
                            "f1":cur_epoch_results["Non0_F1_score"],
                            "mae":cur_epoch_results["MAE"],
                            "corr":cur_epoch_results["Corr"]
                        }
                    cursor.execute(
                        "INSERT INTO EResult (result_id, epoch_num, results) VALUES (%s, %s, %s)",
                        (result_id, i+1, json.dumps(cur_results))
                    )
                db.commit()
                logger.info(f"Task {task_id} Finished.")
            except Exception as e:
                logger.exception(e)
                db.rollback()
                # TODO: remove saved features
                raise e
            cursor.execute("UPDATE Task SET state = %s WHERE task_id = %s", (1, task_id))
        except Exception as e:
            logger.exception(e)
            logger.error(f"Task {task_id} Error.")
            if cursor:
                cursor.execute("UPDATE Task SET state = %s WHERE task_id = %s", (2, task_id))
        finally:
            if cursor:
                cursor.execute("UPDATE Task SET end_time = %s WHERE task_id = %s", (datetime.now(), task_id))
                db.commit()


    def DEMO_run(db_url, feature_file, model_name, dataset_name, result_id, seed):

        db_params = db_url.split('//')[1].split('@')[0].split(':')
        db_user = db_params[0]
        db_pass = db_params[1]
        db_params = db_url.split('//')[1].split('@')[1].split('/')
        db_host = db_params[0]
        db_name = db_params[1]
        # connect to db
        db = mysql.connector.connect(
            user=db_user, password=db_pass, host=db_host, database=db_name
        )
        cursor2 = db.cursor(named_tuple=True)
        cursor2.execute(
            "SELECT * FROM Result WHERE result_id = %s", (result_id,)
        )
        result = cursor2.fetchone()
        save_model_path = result.save_model_path
        assert Path(save_model_path).exists(), f"pkl file {save_model_path} not found."
        result_args = json.loads(result.args)
        args = get_config_regression(model_name, dataset_name)
        args['train_mode'] = 'regression' # backward compatibility. TODO: remove all train_mode in code
        args['cur_seed'] = 1
        args.update(result_args)
        args['feature_T'] = feature_file
        args['feature_A'] = feature_file
        args['feature_V'] = feature_file
        # args['device'] = assign_gpu([])
        args['device'] = 'cpu'
        setup_seed(seed)
        model = AMIO(args).to(args['device'])
        model.load_state_dict(torch.load(save_model_path))
        model.to(args['device'])
        with open(feature_file, 'rb') as f:
            features = pickle.load(f)
        feature_a = torch.Tensor(features['audio']).unsqueeze(0)
        feature_t = torch.Tensor(features['text']).unsqueeze(0)
        feature_v = torch.Tensor(features['video']).unsqueeze(0)
        if 'need_normalized' in args and args['need_normalized']:
            feature_a = torch.mean(feature_a, dim=1, keepdims=True)
            feature_v = torch.mean(feature_v, dim=1, keepdims=True)
        model.eval()
        with torch.no_grad():
            outputs = model(feature_t, feature_a, feature_v)
        predict = round(float(outputs['M'].cpu().detach().squeeze()), 3)
        return predict
