{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "hEkQJYmvQxov",
        "nZAmiT8VwZ8P",
        "FGEg-inKwcTq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clone"
      ],
      "metadata": {
        "id": "NGYqAlZ9RUkt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPu4xitHTcUu",
        "outputId": "4aea69c1-1913-4e06-a025-9e5614638b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into '/content/Modified_Deepmlf'...\n",
            "remote: Enumerating objects: 364, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 364 (delta 54), reused 57 (delta 48), pack-reused 296 (from 1)\u001b[K\n",
            "Receiving objects: 100% (364/364), 404.20 KiB | 26.95 MiB/s, done.\n",
            "Resolving deltas: 100% (198/198), done.\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "\n",
        "import base64\n",
        "\n",
        "encoded = \"bmlja3hpcjEyOmdocF9YY3lEalhNeWxldEpDQTd3eEJ4amVrR1lhWFJ4cTAyOGlFV0g=\"\n",
        "\n",
        "# Decode the string into username/token\n",
        "decoded = base64.b64decode(encoded).decode()\n",
        "username, token = decoded.split(\":\")\n",
        "\n",
        "repo = \"nickxir12/Modified_Deepmlf\"\n",
        "destination = \"/content/Modified_Deepmlf\"\n",
        "\n",
        "# ✅ Go to safe dir before removing anything\n",
        "%cd /content\n",
        "\n",
        "# ✅ Now it's safe to delete\n",
        "!rm -rf {destination}\n",
        "\n",
        "# ✅ Clone the repo\n",
        "#!git clone https://{username}:{token}@github.com/{repo}.git {destination}\n",
        "!git clone --branch efthymis-baseline --single-branch https://{username}:{token}@github.com/{repo}.git {destination}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOSI"
      ],
      "metadata": {
        "id": "hEkQJYmvQxov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !mkdir -p /content/data/mosi/Processed\n",
        "\n",
        "# !cp \"/content/drive/MyDrive/Processed/mosi/unaligned_50.pkl\" \"/content/data/mosi/Processed/\"\n",
        "# !cp \"/content/drive/MyDrive/Processed/mosi/aligned_50.pkl\" \"/content/data/mosi/Processed/\"\n",
        "\n",
        "# import sys\n",
        "# sys.path.append('/content/Modified_Deepmlf')\n",
        "\n",
        "# %cd /content/Modified_Deepmlf/"
      ],
      "metadata": {
        "id": "JA34gv9akqdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-Train Encoder**"
      ],
      "metadata": {
        "id": "nZAmiT8VwZ8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python experiments/regression/mult_base.py \\\n",
        "#   -m bienc \\\n",
        "#   -d mosi \\\n",
        "#   -g 0 \\\n",
        "#   --exp-name mosi-test-bienc \\\n",
        "#   -c MMSA/config/regression/deepmlf/mosi/bienc.json \\\n",
        "#   --res-save-dir MMSA/results/mosi \\\n",
        "#   -n 2 \\\n",
        "#   -s 1990"
      ],
      "metadata": {
        "id": "zaER_-KPwVfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model**"
      ],
      "metadata": {
        "id": "FGEg-inKwcTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python experiments/regression/mult_base.py \\\n",
        "#   -m msalm \\\n",
        "#   -d mosi \\\n",
        "#   -g 0 \\\n",
        "#   --exp-name mosi-test \\\n",
        "#   -c MMSA/config/regression/deepmlf/mosi/best.json \\\n",
        "#   --res-save-dir MMSA/results/mosi \\\n",
        "#   -n 2 \\\n",
        "#   -s 1990"
      ],
      "metadata": {
        "id": "LbDqPVYrwXEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SIMS"
      ],
      "metadata": {
        "id": "9QLFFAxaRG_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/data/sims/Processed\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Processed/sims/unaligned_39.pkl\" \"/content/data/sims/Processed/\"\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/Modified_Deepmlf')\n",
        "\n",
        "%cd /content/Modified_Deepmlf/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RZwPDdzQ4Jz",
        "outputId": "09c27eb9-d348-4aec-fcde-90dde5b6239e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/Modified_Deepmlf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-Train Encoder**"
      ],
      "metadata": {
        "id": "W89NyZdri5Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python experiments/regression/mult_base.py \\\n",
        "#   -m bienc \\\n",
        "#   -d sims \\\n",
        "#   -g 0 \\\n",
        "#   --exp-name sims-test-bienc \\\n",
        "#   -c MMSA/config/regression/deepmlf/sims/bienc.json \\\n",
        "#   --res-save-dir MMSA/results/sims \\\n",
        "#   -n 2 \\\n",
        "#   -s 1990"
      ],
      "metadata": {
        "id": "olHevy3ri47p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model**"
      ],
      "metadata": {
        "id": "hcfEGqwzi2Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python experiments/regression/mult_base.py \\\n",
        "  -m msalm \\\n",
        "  -d sims \\\n",
        "  -g 0 \\\n",
        "  --exp-name sims-test \\\n",
        "  -c MMSA/config/regression/deepmlf/sims/base_best.json \\\n",
        "  --res-save-dir MMSA/results/sims \\\n",
        "  -n 2 \\\n",
        "  -s 1990"
      ],
      "metadata": {
        "id": "JGDxSLsqTkZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c39518-6790-4e9e-fbbe-e8f4204a9fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-10 12:39:51.664198: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-10 12:39:51.681675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754829591.702646   14169 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754829591.709063   14169 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754829591.725311   14169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754829591.725341   14169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754829591.725344   14169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754829591.725346   14169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-10 12:39:51.730076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/Modified_Deepmlf/experiments/regression/../../MMSA/utils/schedulers.py:108: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  params_decay = [p for n, p in model.named_parameters() if n is not 'bias']\n",
            "/content/Modified_Deepmlf/experiments/regression/../../MMSA/utils/schedulers.py:109: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  params_no_decay = [p for n, p in model.named_parameters() if n is 'bias']\n",
            "MMSA - ======================================== Program Start ========================================\n",
            "INFO:MMSA:======================================== Program Start ========================================\n",
            "MMSA - Running with args:\n",
            "INFO:MMSA:Running with args:\n",
            "MMSA - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/content/data/sims/Processed/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 10, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 8, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': '/content/data/sims/Processed/mms2s', 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': 'gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 100, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 20, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': False, 'path_to_pretrained': 'checkpoints/bienc-sims-nat-5e-4-BN-nl-3-d-30/bienc-sims-1993.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0001, 'learning_rate_mmgpt': 0.0001, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}\n",
            "INFO:MMSA:{'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/content/data/sims/Processed/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 10, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 8, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': '/content/data/sims/Processed/mms2s', 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': 'gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 100, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 20, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': False, 'path_to_pretrained': 'checkpoints/bienc-sims-nat-5e-4-BN-nl-3-d-30/bienc-sims-1993.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0001, 'learning_rate_mmgpt': 0.0001, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}\n",
            "MMSA - Seeds: [1990]\n",
            "INFO:MMSA:Seeds: [1990]\n",
            "MMSA - ------------------------------ Running with seed 1990 [1/1] ------------------------------\n",
            "INFO:MMSA:------------------------------ Running with seed 1990 [1/1] ------------------------------\n",
            "MMSA - train samples: (1368,)\n",
            "INFO:MMSA:train samples: (1368,)\n",
            "MMSA - valid samples: (456,)\n",
            "INFO:MMSA:valid samples: (456,)\n",
            "MMSA - test samples: (457,)\n",
            "INFO:MMSA:test samples: (457,)\n",
            "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "ca list is: [5, 6, 7, 8, 9, 10, 11]\n",
            "initializing SoftPerm\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 0\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 1\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 2\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 3\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 4\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 5\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 6\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Parsing decoder block: 0\n",
            "Parsing decoder block: 1\n",
            "Parsing decoder block: 2\n",
            "Parsing decoder block: 3\n",
            "Parsing decoder block: 4\n",
            "Parsing decoder block: 5\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 6\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 7\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 8\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 9\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 10\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 11\n",
            "COpying---------------------------------\n",
            "Using BN_a\n",
            "Using BN_v\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "From Scratch AudioVisual Encoder Initialization\n",
            "------------------ Adding LNorm ------------------------\n",
            "MMSA - The model has 143937589 trainable parameters\n",
            "INFO:MMSA:The model has 143937589 trainable parameters\n",
            "ongoing with msalm\n",
            "5.ca_layer.alpha_1\n",
            "5.ca_layer.alpha_2\n",
            "5.ca_layer.ln_1.weight\n",
            "5.ca_layer.ln_1.bias\n",
            "5.ca_layer.ln_2.weight\n",
            "5.ca_layer.ln_2.bias\n",
            "5.ca_layer.attn.W_q.weight\n",
            "5.ca_layer.attn.W_kv.weight\n",
            "5.ca_layer.attn.W_o.weight\n",
            "5.ca_layer.mlp.c_fc.weight\n",
            "5.ca_layer.mlp.c_fc.bias\n",
            "5.ca_layer.mlp.c_proj.weight\n",
            "5.ca_layer.mlp.c_proj.bias\n",
            "6.ca_layer.alpha_1\n",
            "6.ca_layer.alpha_2\n",
            "6.ca_layer.ln_1.weight\n",
            "6.ca_layer.ln_1.bias\n",
            "6.ca_layer.ln_2.weight\n",
            "6.ca_layer.ln_2.bias\n",
            "6.ca_layer.attn.W_q.weight\n",
            "6.ca_layer.attn.W_kv.weight\n",
            "6.ca_layer.attn.W_o.weight\n",
            "6.ca_layer.mlp.c_fc.weight\n",
            "6.ca_layer.mlp.c_fc.bias\n",
            "6.ca_layer.mlp.c_proj.weight\n",
            "6.ca_layer.mlp.c_proj.bias\n",
            "7.ca_layer.alpha_1\n",
            "7.ca_layer.alpha_2\n",
            "7.ca_layer.ln_1.weight\n",
            "7.ca_layer.ln_1.bias\n",
            "7.ca_layer.ln_2.weight\n",
            "7.ca_layer.ln_2.bias\n",
            "7.ca_layer.attn.W_q.weight\n",
            "7.ca_layer.attn.W_kv.weight\n",
            "7.ca_layer.attn.W_o.weight\n",
            "7.ca_layer.mlp.c_fc.weight\n",
            "7.ca_layer.mlp.c_fc.bias\n",
            "7.ca_layer.mlp.c_proj.weight\n",
            "7.ca_layer.mlp.c_proj.bias\n",
            "8.ca_layer.alpha_1\n",
            "8.ca_layer.alpha_2\n",
            "8.ca_layer.ln_1.weight\n",
            "8.ca_layer.ln_1.bias\n",
            "8.ca_layer.ln_2.weight\n",
            "8.ca_layer.ln_2.bias\n",
            "8.ca_layer.attn.W_q.weight\n",
            "8.ca_layer.attn.W_kv.weight\n",
            "8.ca_layer.attn.W_o.weight\n",
            "8.ca_layer.mlp.c_fc.weight\n",
            "8.ca_layer.mlp.c_fc.bias\n",
            "8.ca_layer.mlp.c_proj.weight\n",
            "8.ca_layer.mlp.c_proj.bias\n",
            "9.ca_layer.alpha_1\n",
            "9.ca_layer.alpha_2\n",
            "9.ca_layer.ln_1.weight\n",
            "9.ca_layer.ln_1.bias\n",
            "9.ca_layer.ln_2.weight\n",
            "9.ca_layer.ln_2.bias\n",
            "9.ca_layer.attn.W_q.weight\n",
            "9.ca_layer.attn.W_kv.weight\n",
            "9.ca_layer.attn.W_o.weight\n",
            "9.ca_layer.mlp.c_fc.weight\n",
            "9.ca_layer.mlp.c_fc.bias\n",
            "9.ca_layer.mlp.c_proj.weight\n",
            "9.ca_layer.mlp.c_proj.bias\n",
            "10.ca_layer.alpha_1\n",
            "10.ca_layer.alpha_2\n",
            "10.ca_layer.ln_1.weight\n",
            "10.ca_layer.ln_1.bias\n",
            "10.ca_layer.ln_2.weight\n",
            "10.ca_layer.ln_2.bias\n",
            "10.ca_layer.attn.W_q.weight\n",
            "10.ca_layer.attn.W_kv.weight\n",
            "10.ca_layer.attn.W_o.weight\n",
            "10.ca_layer.mlp.c_fc.weight\n",
            "10.ca_layer.mlp.c_fc.bias\n",
            "10.ca_layer.mlp.c_proj.weight\n",
            "10.ca_layer.mlp.c_proj.bias\n",
            "11.ca_layer.alpha_1\n",
            "11.ca_layer.alpha_2\n",
            "11.ca_layer.ln_1.weight\n",
            "11.ca_layer.ln_1.bias\n",
            "11.ca_layer.ln_2.weight\n",
            "11.ca_layer.ln_2.bias\n",
            "11.ca_layer.attn.W_q.weight\n",
            "11.ca_layer.attn.W_kv.weight\n",
            "11.ca_layer.attn.W_o.weight\n",
            "11.ca_layer.mlp.c_fc.weight\n",
            "11.ca_layer.mlp.c_fc.bias\n",
            "11.ca_layer.mlp.c_proj.weight\n",
            "11.ca_layer.mlp.c_proj.bias\n",
            "0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Model.W_task.0.weight\n",
            "Model.W_task.0.bias\n",
            "Model.W_task.1.weight\n",
            "Model.W_task.1.bias\n",
            "Model.W_task.3.weight\n",
            "Model.W_task.3.bias\n",
            "Model.W_bn.weight\n",
            "Model.W_bn.bias\n",
            "Model.W_text.weight\n",
            "Model.W_text.bias\n",
            "Model.W_av.weight\n",
            "Model.W_av.bias\n",
            "Model.av_encoder.BN_a.weight\n",
            "Model.av_encoder.BN_a.bias\n",
            "Model.av_encoder.BN_v.weight\n",
            "Model.av_encoder.BN_v.bias\n",
            "Model.av_encoder.proj_a.weight\n",
            "Model.av_encoder.proj_v.weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.0.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.0.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.0.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.0.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.0.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.0.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.0.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.0.norm2.bias\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.1.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.1.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.1.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.1.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.1.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.1.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.1.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.1.norm2.bias\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.2.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.2.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.2.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.2.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.2.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.2.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.2.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.2.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.0.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.0.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.0.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.0.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.0.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.0.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.0.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.0.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.1.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.1.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.1.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.1.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.1.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.1.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.1.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.1.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.2.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.2.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.2.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.2.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.2.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.2.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.2.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.2.norm2.bias\n",
            "Model.av_encoder.fusion.weight\n",
            "Model.av_encoder.fusion.bias\n",
            "Model.av_encoder.clf.weight\n",
            "Model.av_encoder.clf.bias\n",
            "Model.LN.weight\n",
            "Model.LN.bias\n",
            "The total number of trainable parameters is 41.87 M\n",
            "Model.lang_encoder.transformer.wte.0.embedding.weight\n",
            "Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Using grad with decay in Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.wpe.0.positional.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.ln_f.weight\n",
            "Model.lang_encoder.transformer.ln_f.bias\n",
            "Model.W_task.0.weight\n",
            "Using grad with decay in Model.W_task.0.weight\n",
            "Model.W_task.0.bias\n",
            "Using grad with no decay in Model.W_task.0.bias\n",
            "Model.W_task.1.weight\n",
            "Using grad with decay in Model.W_task.1.weight\n",
            "Model.W_task.1.bias\n",
            "Using grad with no decay in Model.W_task.1.bias\n",
            "Model.W_task.3.weight\n",
            "Using grad with decay in Model.W_task.3.weight\n",
            "Model.W_task.3.bias\n",
            "Using grad with no decay in Model.W_task.3.bias\n",
            "Model.W_bn.weight\n",
            "Using grad with decay in Model.W_bn.weight\n",
            "Model.W_bn.bias\n",
            "Using grad with no decay in Model.W_bn.bias\n",
            "Model.W_text.weight\n",
            "Using grad with decay in Model.W_text.weight\n",
            "Model.W_text.bias\n",
            "Using grad with no decay in Model.W_text.bias\n",
            "Model.W_av.weight\n",
            "Using grad with decay in Model.W_av.weight\n",
            "Model.W_av.bias\n",
            "Using grad with no decay in Model.W_av.bias\n",
            "Model.LN.weight\n",
            "Using grad with decay in Model.LN.weight\n",
            "Model.LN.bias\n",
            "Using grad with no decay in Model.LN.bias\n",
            "Will be using warmup for 5 steps\n",
            "  0% 0/43 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Modified_Deepmlf/experiments/regression/mult_base.py\", line 32, in <module>\n",
            "    MMSA_run(\n",
            "  File \"/content/Modified_Deepmlf/experiments/regression/../../MMSA/run.py\", line 390, in MMSA_run\n",
            "    result = _run(args, num_workers, is_tune)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Modified_Deepmlf/experiments/regression/../../MMSA/run.py\", line 445, in _run\n",
            "    epoch_results = trainer.do_train(model, dataloader, return_epoch_results=from_sena)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Modified_Deepmlf/experiments/regression/../../MMSA/trains/singleTask/MSALM.py\", line 419, in do_train\n",
            "    outputs = model(text_ids, audio, vision, attention_mask=attention_mask)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Modified_Deepmlf/experiments/regression/../../MMSA/models/AMIO.py\", line 45, in forward\n",
            "    return self.Model(text_x, audio_x, video_x, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Modified_Deepmlf/experiments/regression/../../MMSA/models/singleTask/MSALM.py\", line 1145, in forward\n",
            "    outputs = self.lang_encoder(\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Modified_Deepmlf/experiments/regression/../../MMSA/models/singleTask/MSALM.py\", line 953, in forward\n",
            "    return super().forward(**kwargs)  # Call the other parent's forward method\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1076, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "                          ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 940, in forward\n",
            "    hidden_states = outputs[0]\n",
            "                    ~~~~~~~^^^\n",
            "TypeError: 'NoneType' object is not subscriptable\n"
          ]
        }
      ]
    }
  ]
}